Over the summer of 2022, I worked with one of my professors on an independent research project focused on machine learning time series analysis. For this project, predicting the future return of individual stocks in the JPX Japanese Stock Market was the focus. The data was provided in a [Kaggle](https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction) competition. The aim of this project was to get comfortable with more advanced machine learning techniques for time series forecasting, specifically long short-term memory recurrent neural networks. My professor also introduced me to concepts such as ARIMA models and markov chains, but the actual model I built was a long short-term memory recurrent neural network. 

We first started with an [introduction to time series analysis](tsa_rnn_intro.ipynb) in python. After getting comfortable with some of the concepts, I started focusing on preprocessing and preparing the data, planning the project's steps so I could bring clarity to the problem. At the end of the project, I had developed a [model](jpx_func.ipynb) I was confident in which was able to apply a recurrent neural network model to historical data for each individual stock and use that model to predict the top 200 stocks with highest Sharpe ratio, thus the best stocks to invest in, x days in the future. Overall, I was satisfied with the results of the model, but I never actually sent in a submission to the Kaggle competition because of the competition's requirements (Linux OS and a file that was not run on the internet were required. I used macOS and used Google Colab to write code). This project also further heightened my interest with time series analysis, especially with financial data, leading to another small independent research project.

Currently, I have applied some of the methods I learned in this research project to the NYSE. Using altered scaling metrics, but a similar overall model, I developed code that returns the top 10 stocks to invest in daily based off of the list of stocks in the S&P 500. The results have been a mix, and I have been constantly trying new methods to raise the signal-to-noise ratio. 

Overall, the recurrent neural network has been promising, but hyperparameter optimization is tedious, as each individual stock has its own respective sequential model. The hyperparameters are the same for all stocks at this moment, using the Adam optimizer while incorporating a couple LSTM layers, a couple dropout layers, and a couple dense layers. Looking forward, I am working on developing a DenseNet-LSTM implementation, not just for this specific financial data, but for data analysis in general.
